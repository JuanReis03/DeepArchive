# DeepArchive

**DeepArchive** √© um sistema de busca sem√¢ntica inteligente projetado para aprimorar instrumentos de busca em arquivos permanentes. Este projeto, parte da minha pesquisa cient√≠fica do grupo LTI DIGITAL, utiliza modelos de linguagem de larga escala (LLMs) para ir al√©m da busca tradicional por palavras-chave.

O objetivo √© criar um "servi√ßo de refer√™ncia digital" capaz de compreender o **contexto** e a **inten√ß√£o** por tr√°s de uma consulta, retornando resultados semanticamente relevantes, mesmo que os termos exatos n√£o estejam presentes no documento.

## üíª Tecnologias Utilizadas

* **Python 3.12+**
* **DeepSeek (via Ollama):** O modelo de LLM usado para gerar os *embeddings* (representa√ß√µes vetoriais) dos textos.
* **Ollama:** Ferramenta para servir e gerenciar os modelos LLM localmente.
* **LangChain:** O framework principal para construir o *pipeline* de processamento (carregar, dividir, indexar, consultar).
* **BM25 (Rank_BM25):** Algoritmo para busca por palavras-chave (Sparse Retrieval).
* **Docx2txt / PyPDF:** Processamento e ingest√£o de arquivos.
* **ChromaDB:** O banco de dados vetorial de c√≥digo aberto usado para armazenar e consultar os *embeddings*.

## Fase 1 - Conclu√≠da

O *pipeline* central do DeepArchive est√° 100% funcional. O que j√° foi implementado:

* **Indexa√ß√£o de Documentos (`index.py`):**
    * Carregamento autom√°tico de todos os arquivos `.pdf` da pasta `/data`.
    * Fragmenta√ß√£o (*chunking*) dos textos em segmentos otimizados.
    * Gera√ß√£o de *embeddings* usando o modelo **DeepSeek (`deepseek-llm`)** servido localmente via Ollama.
    * Persist√™ncia dos vetores em um banco de dados **ChromaDB** local (na pasta `/db`).

* **Consulta Sem√¢ntica (`query.py`):**
    * Um script de console interativo que recebe perguntas do usu√°rio.
    * Gera√ß√£o de *embedding* para a consulta usando o mesmo modelo DeepSeek (garantindo consist√™ncia).
    * Realiza√ß√£o da busca por similaridade (k=3) no ChromaDB, retornando os *chunks* de texto mais relevantes.
    * Exibi√ß√£o dos resultados com o conte√∫do e a fonte (nome do arquivo e p√°gina).

## Status Atual (Fase 2 - RAG & Interface H√≠brida)

O projeto evoluiu de um simples buscador para um **Assistente Inteligente Completo**. As funcionalidades atuais incluem:

* **Ingest√£o Multiformato (`index.py`):**
    * Suporte para leitura e processamento de arquivos **.pdf** e **.docx** (Word).
    * Limpeza autom√°tica do banco de dados antigo antes da reindexa√ß√£o.
    * Monitoramento de tempo de processamento.

* **Busca H√≠brida (Hybrid Search):**
    * Combina a precis√£o da busca por palavras-chave (**BM25**) com o entendimento contextual da busca vetorial (**ChromaDB**).
    * Utiliza *Ensemble Retriever* para garantir que termos t√©cnicos exatos e conceitos abstratos sejam encontrados com igual efici√™ncia.

* **Pipeline RAG (`app.py`):**
    * O sistema n√£o apenas busca, mas **l√™** os documentos e **responde** √† pergunta do usu√°rio.
    * Cita√ß√£o expl√≠cita das fontes consultadas ao final da resposta.

## ‚öôÔ∏è Como Executar o Projeto

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [URL-DO-SEU-REPOSIT√ìRIO]
    cd busca-de-arquivos-IC
    ```

2.  **Instale o Ollama:**
    * Baixe e instale o [Ollama](https://ollama.com/) no seu sistema.
    * Puxe o modelo DeepSeek que ser√° usado:
        ```bash
        ollama pull deepseek-llm
        ```

3.  **Crie e ative o ambiente virtual (venv):**
    ```bash
    python -m venv venv
    .\venv\Scripts\activate
    ```
    *(No Linux/Mac: `source venv/bin/activate`)*

4.  **Instale as depend√™ncias:**
    ```bash
    # Primeiro, PyTorch com suporte a CUDA
    pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
    
    # Depois, as bibliotecas do projeto
    pip install -U langchain langchain-community pypdf langchain-chroma langchain-ollama
    ```

5.  **Adicione seus arquivos:**
    * Coloque os arquivos `.pdf` que deseja indexar dentro da pasta `/data`.

6.  **Execute a Indexa√ß√£o (Apenas uma vez):**
    * Este script processar√° os arquivos da pasta `/data` e criar√° o banco `db`.
        ```bash
        python index.py
        ```

7.  **Execute a Consulta:**
    * Inicie o script de busca interativo.
        ```bash
        python query.py
        ```
## üó∫Ô∏è Planos Futuros (Roadmap)

Com a funda√ß√£o do RAG e da interface estabelecida, os pr√≥ximos passos focam em robustez e funcionalidades avan√ßadas:

* **Melhorias na Ingest√£o de Dados:**
    * Implementar **OCR** para extrair texto de PDFs baseados em imagem (documentos digitalizados antigos).

* **Melhorias na Busca e IA:**
    * Implementar **filtragem por metadados** (ex: permitir que o usu√°rio filtre a busca por ano ou autor antes de perguntar).
    * Refinamento dos *prompts* do sistema para diferentes perfis de resposta (ex: "Modo Resumo" vs "Modo Detalhado").
    
* **Melhorias na Interface (UX):**
    * **[Prototipagem R√°pida]** Substituir o `query.py` por uma interface web usando **Streamlit**.
    * Agrupar resultados da busca por arquivo de origem, exibindo as p√°ginas relevantes (ex: "Arquivo X: p√°gs 2, 5, 10").

* **N√≠vel de Produ√ß√£o (Deploy):**
    * **[Containeriza√ß√£o]** Criar um `Dockerfile` e `docker-compose.yml` para empacotar a aplica√ß√£o.
    * Estabelecer um *pipeline* de avalia√ß√£o automatizada para medir a precis√£o das respostas geradas.
